# Hull Tactical Market Prediction - å®Œæ•´éƒ¨ç½²æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### ç³»ç»Ÿè¦æ±‚
- Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- è‡³å°‘ 8GB RAM
- ï¼ˆå¯é€‰ï¼‰NVIDIA GPU ç”¨äºåŠ é€Ÿè®­ç»ƒ

#### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨ condaï¼ˆæ¨èï¼‰
conda create -n hull-prediction python=3.9
conda activate hull-prediction

# æˆ–ä½¿ç”¨ venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows
```

### 2. å®‰è£…ä¾èµ–

```bash
# å…‹éš†æˆ–åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir hull-tactical-prediction
cd hull-tactical-prediction

# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# æˆ–æ‰‹åŠ¨å®‰è£…æ ¸å¿ƒä¾èµ–
pip install pandas numpy scikit-learn lightgbm xgboost catboost matplotlib seaborn pyyaml joblib
```

**requirements.txt å†…å®¹ï¼š**
```txt
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
lightgbm>=3.3.0
xgboost>=1.5.0
catboost>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
pyyaml>=5.4.0
joblib>=1.1.0
scipy>=1.7.0
tqdm>=4.62.0
```

### 3. é¡¹ç›®åˆå§‹åŒ–

```bash
# åˆ›å»ºç›®å½•ç»“æ„
python setup.py init

# æˆ–æ‰‹åŠ¨åˆ›å»º
mkdir -p data/raw data/processed src outputs/{submissions,models,figures,logs} config notebooks scripts tests
```

### 4. æ•°æ®å‡†å¤‡

```bash
# å°† Kaggle æ•°æ®é›†ä¸‹è½½åˆ° data/raw/
# æ–¹æ³•1: ä½¿ç”¨ Kaggle API
kaggle competitions download -c hull-tactical-market-prediction -p data/raw/
unzip data/raw/hull-tactical-market-prediction.zip -d data/raw/

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½
# è®¿é—® https://www.kaggle.com/competitions/hull-tactical-market-prediction/data
# ä¸‹è½½ train.csv å’Œ test.csv åˆ° data/raw/
```

### 5. é…ç½®æ–‡ä»¶è®¾ç½®

åˆ›å»º `config/config.yaml`:

```yaml
# æ•°æ®è·¯å¾„
data:
  raw_path: './data/raw/'
  processed_path: './data/processed/'
  train_file: 'train.csv'
  test_file: 'test.csv'

# ç‰¹å¾å·¥ç¨‹é…ç½®
feature_engineering:
  missing_strategy: 'median'  # median, mean, forward_fill
  create_lag_features: true
  lag_periods: [1, 2, 3, 5, 10, 20]
  create_rolling_features: true
  rolling_windows: [5, 10, 20, 30, 60]
  create_momentum_features: true
  momentum_periods: [5, 10, 20]
  
# æ¨¡å‹é…ç½®
model:
  cv_splits: 5
  random_state: 42
  models_to_train: ['ridge', 'lasso', 'lgbm', 'xgb', 'catboost']
  
# é›†æˆé…ç½®
ensemble:
  strategy: 'weighted_average'  # simple_average, weighted_average, median, rank_average
  
# è¾“å‡ºé…ç½®
output:
  save_models: true
  save_predictions: true
  create_figures: true
  
# æ—¥å¿—é…ç½®
logging:
  level: 'INFO'
  file: './outputs/logs/training.log'
```

## ğŸ“Š ä½¿ç”¨æµç¨‹

### æ–¹å¼ 1: ä½¿ç”¨ä¸»è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# å®Œæ•´æµç¨‹ï¼šæ•°æ®åŠ è½½ -> ç‰¹å¾å·¥ç¨‹ -> è®­ç»ƒ -> é¢„æµ‹ -> æäº¤
python run_pipeline.py --config config/config.yaml

# æŒ‡å®šç‰¹å®šæ­¥éª¤
python run_pipeline.py --steps data,feature,train
python run_pipeline.py --steps predict,submit

# å¿«é€Ÿæµ‹è¯•ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰
python run_pipeline.py --quick
```

### æ–¹å¼ 2: åˆ†æ­¥æ‰§è¡Œ

```bash
# 1. æ•°æ®æ¢ç´¢
python scripts/eda.py

# 2. è®­ç»ƒæ¨¡å‹
python scripts/train.py --config config/config.yaml

# 3. ç”Ÿæˆé¢„æµ‹
python scripts/predict.py --model-path outputs/models/best_model.pkl

# 4. åˆ›å»ºæäº¤æ–‡ä»¶
python scripts/submit.py --predictions outputs/predictions.csv
```

### æ–¹å¼ 3: ä½¿ç”¨ Jupyter Notebook

```bash
jupyter notebook

# ä¾æ¬¡è¿è¡Œï¼š
# notebooks/01_eda.ipynb
# notebooks/02_feature_engineering.ipynb
# notebooks/03_model_training.ipynb
```

### æ–¹å¼ 4: Python äº¤äº’å¼ä½¿ç”¨

```python
from src.data_loader import DataLoader
from src.feature_engineering import FeatureEngineering
from src.model_builder import ModelBuilder
from src.ensemble import EnsembleModel

# åŠ è½½æ•°æ®
loader = DataLoader(data_path='./data/raw/')
train_df, test_df = loader.load_data()
features, target = loader.prepare_features()

# ç‰¹å¾å·¥ç¨‹
fe = FeatureEngineering(train_df, test_df, features)
train_df, test_df = fe.handle_missing_values()

# è®­ç»ƒæ¨¡å‹
builder = ModelBuilder(train_df, features, target)
X, y = builder.prepare_data()
models = builder.train_final_models(X, y)

# é¢„æµ‹å’Œé›†æˆ
predictions = builder.predict(test_df)
ensemble = EnsembleModel(predictions)
final_pred = ensemble.weighted_average()
```

## ğŸ¯ è°ƒä¼˜æŒ‡å—

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºçº¿å»ºç«‹ï¼ˆ1-2å°æ—¶ï¼‰

**ç›®æ ‡ï¼š**å»ºç«‹å¯å·¥ä½œçš„åŸºçº¿æ¨¡å‹

```bash
# 1. è¿è¡Œé»˜è®¤é…ç½®
python run_pipeline.py --quick

# 2. æŸ¥çœ‹äº¤å‰éªŒè¯ç»“æœ
cat outputs/logs/cv_scores.txt

# 3. æäº¤åŸºçº¿ç»“æœ
# é€‰æ‹©è¡¨ç°æœ€å¥½çš„æ¨¡å‹æäº¤åˆ° Kaggle
```

**é¢„æœŸç»“æœï¼š**
- è·å¾—åˆå§‹ leaderboard åˆ†æ•°
- äº†è§£æ•°æ®ç‰¹æ€§
- è¯†åˆ«é—®é¢˜å’Œæ”¹è¿›æ–¹å‘

---

### ç¬¬äºŒé˜¶æ®µï¼šç‰¹å¾å·¥ç¨‹ï¼ˆ2-4å°æ—¶ï¼‰

**ä¼˜å…ˆçº§ï¼šâ­â­â­â­â­**

#### 2.1 æ»åç‰¹å¾ä¼˜åŒ–

ä¿®æ”¹ `config/config.yaml`:

```yaml
feature_engineering:
  create_lag_features: true
  lag_periods: [1, 2, 3, 5, 7, 10, 14, 20, 30]  # å¢åŠ æ›´å¤šæ»åæœŸ
```

è¿è¡Œï¼š
```bash
python run_pipeline.py --steps feature,train --config config/config.yaml
```

**è°ƒå‚å»ºè®®ï¼š**
- çŸ­æœŸæ»å (1-5)ï¼šæ•æ‰è¿‘æœŸè¶‹åŠ¿
- ä¸­æœŸæ»å (7-20)ï¼šæ•æ‰å‘¨æœŸæ€§
- é•¿æœŸæ»å (30+)ï¼šæ•æ‰é•¿æœŸè¶‹åŠ¿

#### 2.2 æ»šåŠ¨çª—å£ç‰¹å¾

```yaml
feature_engineering:
  create_rolling_features: true
  rolling_windows: [3, 5, 7, 10, 15, 20, 30, 60, 90]
  rolling_stats: ['mean', 'std', 'min', 'max']  # å¢åŠ æ›´å¤šç»Ÿè®¡é‡
```

**è°ƒå‚å»ºè®®ï¼š**
- å°è¯•ä¸åŒçª—å£å¤§å°
- æ·»åŠ æ›´å¤šç»Ÿè®¡é‡ï¼ˆä¸­ä½æ•°ã€åˆ†ä½æ•°ã€ååº¦ï¼‰
- è®¡ç®—å¤šä¸ªç‰¹å¾çš„æ»šåŠ¨ç»Ÿè®¡

#### 2.3 è‡ªå®šä¹‰ç‰¹å¾åˆ›å»º

ç¼–è¾‘ `src/feature_engineering.py`ï¼Œæ·»åŠ ï¼š

```python
def create_custom_features(self):
    """åˆ›å»ºè‡ªå®šä¹‰é‡‘èç‰¹å¾"""
    # æ³¢åŠ¨ç‡ç‰¹å¾
    for window in [5, 10, 20]:
        self.train_df[f'volatility_{window}'] = (
            self.train_df['close'].rolling(window).std()
        )
    
    # ä»·æ ¼å˜åŒ–ç‡
    for period in [1, 5, 10]:
        self.train_df[f'return_{period}'] = (
            self.train_df['close'].pct_change(period)
        )
    
    # æŠ€æœ¯æŒ‡æ ‡
    # RSI, MACD, Bollinger Bands ç­‰
    ...
```

**é¢„æœŸæå‡ï¼š**0.5-2% æ€§èƒ½æ”¹è¿›

---

### ç¬¬ä¸‰é˜¶æ®µï¼šæ¨¡å‹è°ƒä¼˜ï¼ˆ3-6å°æ—¶ï¼‰

**ä¼˜å…ˆçº§ï¼šâ­â­â­â­**

#### 3.1 LightGBM è°ƒä¼˜ï¼ˆæ¨èé‡ç‚¹ï¼‰

åˆ›å»º `config/lgbm_params.yaml`:

```yaml
# ç¬¬ä¸€è½®ï¼šç²—è°ƒ
lgbm_round1:
  n_estimators: [100, 200, 300, 500]
  learning_rate: [0.01, 0.05, 0.1]
  max_depth: [3, 5, 7]
  num_leaves: [31, 63, 127]

# ç¬¬äºŒè½®ï¼šç²¾è°ƒï¼ˆåŸºäºç¬¬ä¸€è½®æœ€ä½³å‚æ•°ï¼‰
lgbm_round2:
  n_estimators: [400, 500, 600]      # å›´ç»•æœ€ä½³å€¼
  learning_rate: [0.08, 0.1, 0.12]   # å›´ç»•æœ€ä½³å€¼
  max_depth: [6, 7, 8]               # å›´ç»•æœ€ä½³å€¼
  num_leaves: [50, 63, 80]           # å›´ç»•æœ€ä½³å€¼
  min_child_samples: [10, 20, 30]
  subsample: [0.7, 0.8, 0.9]
  colsample_bytree: [0.7, 0.8, 0.9]
  reg_alpha: [0, 0.1, 0.5]
  reg_lambda: [0, 0.5, 1.0]
```

è¿è¡Œè°ƒä¼˜ï¼š

```bash
# ç²—è°ƒï¼ˆå¿«é€Ÿï¼‰
python scripts/tune_model.py --model lgbm --params config/lgbm_params.yaml --round 1 --n-iter 20

# ç²¾è°ƒï¼ˆåŸºäºç²—è°ƒç»“æœï¼‰
python scripts/tune_model.py --model lgbm --params config/lgbm_params.yaml --round 2 --n-iter 50
```

**æ—¶é—´æˆæœ¬ï¼š**
- ç²—è°ƒï¼š30-60åˆ†é’Ÿ
- ç²¾è°ƒï¼š2-3å°æ—¶

**é¢„æœŸæå‡ï¼š**1-3% æ€§èƒ½æ”¹è¿›

#### 3.2 XGBoost è°ƒä¼˜

```bash
python scripts/tune_model.py --model xgb --n-iter 30
```

#### 3.3 å¤šæ¨¡å‹å¯¹æ¯”

```python
# åœ¨ scripts/train.py ä¸­
python scripts/train.py --models lgbm,xgb,catboost,rf --compare
```

æŸ¥çœ‹å¯¹æ¯”ç»“æœï¼š
```bash
cat outputs/logs/model_comparison.txt
```

---

### ç¬¬å››é˜¶æ®µï¼šé›†æˆä¼˜åŒ–ï¼ˆ1-2å°æ—¶ï¼‰

**ä¼˜å…ˆçº§ï¼šâ­â­â­â­**

#### 4.1 åŸºäºCVåˆ†æ•°çš„åŠ æƒé›†æˆ

ç¼–è¾‘ `src/ensemble.py`:

```python
def calculate_optimal_weights(self, cv_scores):
    """åŸºäºCVåˆ†æ•°è®¡ç®—æƒé‡"""
    # ä½¿ç”¨ RMSE çš„å€’æ•°ä½œä¸ºæƒé‡
    scores = np.array([cv_scores[model]['mean_rmse'] for model in self.predictions.keys()])
    weights = 1.0 / scores
    weights = weights / weights.sum()
    return weights
```

è¿è¡Œï¼š
```bash
python scripts/ensemble.py --strategy weighted --use-cv-scores
```

#### 4.2 Stacking é›†æˆ

```python
# ä½¿ç”¨ scripts/stacking.py
python scripts/stacking.py --base-models lgbm,xgb,catboost --meta-model ridge
```

#### 4.3 å¤šå±‚é›†æˆ

```bash
# Level 1: å¤šä¸ªæ¨¡å‹
# Level 2: é›†æˆ Level 1 çš„é¢„æµ‹
python scripts/multi_level_ensemble.py
```

**é¢„æœŸæå‡ï¼š**0.5-1.5% æ€§èƒ½æ”¹è¿›

---

### ç¬¬äº”é˜¶æ®µï¼šé«˜çº§ä¼˜åŒ–ï¼ˆ2-4å°æ—¶ï¼‰

**ä¼˜å…ˆçº§ï¼šâ­â­â­**

#### 5.1 ç‰¹å¾é€‰æ‹©

```bash
# åŸºäºé‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©
python scripts/feature_selection.py --method importance --top-k 100

# é€’å½’ç‰¹å¾æ¶ˆé™¤
python scripts/feature_selection.py --method rfe --n-features 50

# ç›¸å…³æ€§è¿‡æ»¤
python scripts/feature_selection.py --method correlation --threshold 0.95
```

#### 5.2 äº¤å‰éªŒè¯ç­–ç•¥ä¼˜åŒ–

```yaml
# config/config.yaml
model:
  cv_strategy: 'time_series'  # time_series, kfold, purged
  cv_splits: 10  # å¢åŠ æŠ˜æ•°ä»¥è·å¾—æ›´ç¨³å®šçš„éªŒè¯
  purge_gap: 5   # å¯¹äº purged CV
```

#### 5.3 æ•°æ®å¢å¼º

```python
# åœ¨ src/feature_engineering.py ä¸­æ·»åŠ 
def augment_data(self):
    """æ•°æ®å¢å¼º"""
    # æ·»åŠ å™ªå£°
    # æ—¶é—´åºåˆ—bootstrap
    # SMOTEï¼ˆå¦‚æœé€‚ç”¨ï¼‰
    ...
```

---

### ç¬¬å…­é˜¶æ®µï¼šæœ€ç»ˆä¼˜åŒ–ï¼ˆ1-2å°æ—¶ï¼‰

**ä¼˜å…ˆçº§ï¼šâ­â­**

#### 6.1 è¶…å‚æ•°å¾®è°ƒ

åŸºäºå‰é¢çš„æœ€ä½³æ¨¡å‹ï¼Œè¿›è¡Œæœ€åçš„å¾®è°ƒï¼š

```bash
python scripts/final_tune.py --model best_lgbm --fine-tune --n-iter 100
```

#### 6.2 æ¨¡å‹èåˆ

```bash
# èåˆå¤šä¸ªæœ€ä½³æ¨¡å‹çš„é¢„æµ‹
python scripts/blend_models.py --models model1.pkl,model2.pkl,model3.pkl --weights 0.4,0.4,0.2
```

#### 6.3 åå¤„ç†ä¼˜åŒ–

```python
# åœ¨ scripts/postprocess.py ä¸­
def postprocess_predictions(predictions):
    """é¢„æµ‹åå¤„ç†"""
    # å‰ªè£å¼‚å¸¸å€¼
    predictions = np.clip(predictions, lower_bound, upper_bound)
    
    # å¹³æ»‘å¤„ç†
    predictions = smooth_predictions(predictions, window=3)
    
    return predictions
```

---

## ğŸ“ˆ è°ƒä¼˜ç›‘æ§

### è·Ÿè¸ªå®éªŒç»“æœ

åˆ›å»ºå®éªŒæ—¥å¿—ï¼š

```python
# experiments_log.csv
experiment_id,date,features,model,params,cv_score,lb_score,notes
exp_001,2025-11-04,baseline,lgbm,default,0.0156,0.0162,baseline
exp_002,2025-11-04,+lag_features,lgbm,default,0.0149,0.0155,added lag 1-10
exp_003,2025-11-04,+lag_features,lgbm,tuned,0.0142,0.0148,tuned lgbm
...
```

### ä½¿ç”¨å®éªŒè¿½è¸ªå·¥å…·

```bash
# å®‰è£… MLflow
pip install mlflow

# å¯åŠ¨ MLflow UI
mlflow ui

# åœ¨è®­ç»ƒè„šæœ¬ä¸­è®°å½•å®éªŒ
python scripts/train.py --use-mlflow
```

### å¯è§†åŒ–æ”¹è¿›

```python
import matplotlib.pyplot as plt
import pandas as pd

# è¯»å–å®éªŒæ—¥å¿—
df = pd.read_csv('experiments_log.csv')

# ç»˜åˆ¶è¿›åº¦æ›²çº¿
plt.figure(figsize=(12, 6))
plt.plot(df['experiment_id'], df['cv_score'], marker='o', label='CV Score')
plt.plot(df['experiment_id'], df['lb_score'], marker='s', label='LB Score')
plt.xlabel('Experiment')
plt.ylabel('Score (RMSE)')
plt.title('Model Performance Over Experiments')
plt.legend()
plt.grid(True)
plt.savefig('outputs/figures/progress.png')
```

---

## ğŸ” è°ƒè¯•æŠ€å·§

### å¸¸è§é—®é¢˜æ’æŸ¥

#### 1. æœ¬åœ°CVä¸çº¿ä¸ŠLBä¸ä¸€è‡´

**å¯èƒ½åŸå› ï¼š**
- æ•°æ®æ³„æ¼ï¼ˆç‰¹å¾å·¥ç¨‹ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯ï¼‰
- CVåˆ’åˆ†æ–¹å¼ä¸å½“
- è¿‡æ‹Ÿåˆ

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# æ£€æŸ¥æ•°æ®æ³„æ¼
python scripts/check_leakage.py

# ä½¿ç”¨æ›´ä¸¥æ ¼çš„CV
cv = TimeSeriesSplit(n_splits=10)

# å¢åŠ æ­£åˆ™åŒ–
lgbm_params['reg_alpha'] = 1.0
lgbm_params['reg_lambda'] = 1.0
```

#### 2. è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**ä¼˜åŒ–æ–¹æ¡ˆï¼š**
```python
# 1. ä½¿ç”¨GPU
lgbm_params['device'] = 'gpu'
xgb_params['tree_method'] = 'gpu_hist'

# 2. å‡å°‘ç‰¹å¾æ•°é‡
python scripts/feature_selection.py --top-k 50

# 3. ä½¿ç”¨æ›´å°‘çš„CVæŠ˜æ•°
cv_splits = 3

# 4. å¹¶è¡Œè®­ç»ƒ
n_jobs = -1
```

#### 3. å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# ä½¿ç”¨æ•°æ®ç±»å‹ä¼˜åŒ–
def reduce_mem_usage(df):
    for col in df.columns:
        col_type = df[col].dtype
        if col_type != object:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                # ... å…¶ä»–ç±»å‹
    return df

# åˆ†æ‰¹å¤„ç†
chunk_size = 10000
for chunk in pd.read_csv('train.csv', chunksize=chunk_size):
    process_chunk(chunk)
```

---

## ğŸ“ æœ€ä½³å®è·µ

### 1. ç‰ˆæœ¬æ§åˆ¶

```bash
git init
git add .
git commit -m "Initial commit"

# ä¸ºæ¯æ¬¡é‡è¦æ”¹è¿›åˆ›å»ºåˆ†æ”¯
git checkout -b feature/lag-features
git checkout -b experiment/lgbm-tuning
```

### 2. ä»£ç å¤ç”¨

å°†æˆåŠŸçš„ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é…ç½®ä¿å­˜ä¸ºæ¨¡æ¿ï¼š

```python
# templates/successful_features.py
BEST_LAG_CONFIG = {
    'periods': [1, 2, 3, 5, 10, 20],
    'features': ['feature_1', 'feature_2', ...]
}

BEST_ROLLING_CONFIG = {
    'windows': [5, 10, 20, 30],
    'stats': ['mean', 'std', 'min', 'max']
}
```

### 3. è‡ªåŠ¨åŒ–æµ‹è¯•

```python
# tests/test_features.py
def test_no_future_leakage():
    """ç¡®ä¿ç‰¹å¾ä¸åŒ…å«æœªæ¥ä¿¡æ¯"""
    fe = FeatureEngineering(train_df, test_df, features)
    fe.create_lag_features(columns, lags=[1, 2, 3])
    
    # æ£€æŸ¥æ¯ä¸ªæ»åç‰¹å¾
    for col in fe.train_df.columns:
        if 'lag' in col:
            assert not has_future_info(fe.train_df[col])

def test_no_data_leakage():
    """ç¡®ä¿trainå’Œtestæ²¡æœ‰ä¿¡æ¯æ³„æ¼"""
    assert len(set(train_df.index) & set(test_df.index)) == 0
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
pytest tests/
```

### 4. æ–‡æ¡£è®°å½•

ä¸ºæ¯ä¸ªå®éªŒè®°å½•è¯¦ç»†ä¿¡æ¯ï¼š

```markdown
## Experiment 015 - 2025-11-04

### æ”¹åŠ¨
- æ·»åŠ äº†æ³¢åŠ¨ç‡ç‰¹å¾ (5, 10, 20æ—¥çª—å£)
- LightGBM: learning_rate=0.08, n_estimators=500

### ç»“æœ
- CV RMSE: 0.0142 (â†“ 0.0007)
- LB RMSE: 0.0148 (â†“ 0.0005)

### åˆ†æ
- æ³¢åŠ¨ç‡ç‰¹å¾è´¡çŒ®åº¦è¾ƒé«˜ (feature importance top 5)
- éªŒè¯é›†å’Œæµ‹è¯•é›†è¡¨ç°ä¸€è‡´ï¼Œæœªè¿‡æ‹Ÿåˆ

### ä¸‹ä¸€æ­¥
- å°è¯•æ·»åŠ æ›´å¤šæŠ€æœ¯æŒ‡æ ‡
- è¿›ä¸€æ­¥è°ƒä¼˜ num_leaves å‚æ•°
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### é¢„æœŸæ”¹è¿›è·¯å¾„

| é˜¶æ®µ | æ“ä½œ | CVæå‡ | LBæå‡ | æ—¶é—´æŠ•å…¥ |
|------|------|--------|--------|----------|
| åŸºçº¿ | é»˜è®¤é…ç½® | 0.0200 | 0.0205 | 1h |
| ç‰¹å¾å·¥ç¨‹ | æ»å+æ»šåŠ¨ç‰¹å¾ | -0.0030 | -0.0025 | 3h |
| æ¨¡å‹è°ƒä¼˜ | LightGBMè°ƒå‚ | -0.0020 | -0.0018 | 4h |
| é›†æˆä¼˜åŒ– | åŠ æƒé›†æˆ | -0.0015 | -0.0012 | 2h |
| é«˜çº§ä¼˜åŒ– | ç‰¹å¾é€‰æ‹©+Stacking | -0.0010 | -0.0008 | 3h |
| **æ€»è®¡** | | **0.0125** | **0.0142** | **13h** |

---

## ğŸ› ï¸ æ•…éšœæ’é™¤

### ç¯å¢ƒé—®é¢˜

```bash
# é—®é¢˜ï¼šLightGBMå®‰è£…å¤±è´¥
# è§£å†³ï¼š
conda install -c conda-forge lightgbm

# é—®é¢˜ï¼šXGBoost GPUä¸å¯ç”¨
# è§£å†³ï¼š
pip install xgboost-gpu

# é—®é¢˜ï¼šä¾èµ–å†²çª
# è§£å†³ï¼š
pip install --upgrade pip
pip install -r requirements.txt --force-reinstall
```

### æ•°æ®é—®é¢˜

```bash
# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
python scripts/validate_data.py

# ä¿®å¤æŸåçš„CSV
python scripts/fix_csv.py
```

---

## ğŸ“ è·å–å¸®åŠ©

### èµ„æºé“¾æ¥

- **Kaggle Competition**: https://www.kaggle.com/competitions/hull-tactical-market-prediction
- **Discussion Forum**: https://www.kaggle.com/competitions/hull-tactical-market-prediction/discussion
- **LightGBMæ–‡æ¡£**: https://lightgbm.readthedocs.io/
- **XGBoostæ–‡æ¡£**: https://xgboost.readthedocs.io/

### ç¤¾åŒºæ”¯æŒ

- Kaggle Discussion å‘å¸–æé—®
- GitHub Issuesï¼ˆå¦‚æœä»£ç æœ‰é—®é¢˜ï¼‰

---

## âœ… æ£€æŸ¥æ¸…å•

éƒ¨ç½²å‰æ£€æŸ¥ï¼š
- [ ] å®‰è£…æ‰€æœ‰ä¾èµ–
- [ ] æ•°æ®æ–‡ä»¶åœ¨æ­£ç¡®ä½ç½®
- [ ] é…ç½®æ–‡ä»¶å·²åˆ›å»º
- [ ] ç›®å½•ç»“æ„å®Œæ•´
- [ ] å¯ä»¥è¿è¡Œ `python run_pipeline.py --quick`

æäº¤å‰æ£€æŸ¥ï¼š
- [ ] æäº¤æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ˆid, targetä¸¤åˆ—ï¼‰
- [ ] æ²¡æœ‰NaNæˆ–Infå€¼
- [ ] IDä¸test.csvå®Œå…¨åŒ¹é…
- [ ] é¢„æµ‹å€¼èŒƒå›´åˆç†
- [ ] å·²åœ¨æœ¬åœ°éªŒè¯

---

## ğŸ¯ æˆåŠŸè·¯çº¿å›¾

**ç¬¬1å¤©ï¼ˆ4å°æ—¶ï¼‰**
- âœ… ç¯å¢ƒæ­å»ºå’Œæ•°æ®åŠ è½½
- âœ… åŸºçº¿æ¨¡å‹å»ºç«‹
- âœ… é¦–æ¬¡æäº¤

**ç¬¬2-3å¤©ï¼ˆ8å°æ—¶ï¼‰**
- ğŸ¨ ç‰¹å¾å·¥ç¨‹è¿­ä»£
- ğŸ”§ LightGBMè°ƒä¼˜
- ğŸ“Š äº¤å‰éªŒè¯ä¼˜åŒ–

**ç¬¬4-5å¤©ï¼ˆ8å°æ—¶ï¼‰**
- ğŸ¤ é›†æˆå­¦ä¹ 
- ğŸ¯ ç‰¹å¾é€‰æ‹©
- ğŸš€ æœ€ç»ˆä¼˜åŒ–

**ç¬¬6-7å¤©ï¼ˆ4å°æ—¶ï¼‰**
- ğŸ“ˆ å®éªŒåˆ†æ
- ğŸ† æäº¤æœ€ä½³æ¨¡å‹
- ğŸ“ æ€»ç»“æ–‡æ¡£

**æ€»æ—¶é—´æŠ•å…¥ï¼š24å°æ—¶**
**é¢„æœŸæ’åï¼šTop 10-20%**

---

ç¥ä½ åœ¨æ¯”èµ›ä¸­å–å¾—ä¼˜å¼‚æˆç»©ï¼ğŸ†

å¦‚æœé‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤éƒ¨åˆ†æˆ–åœ¨ Discussion ä¸­æé—®ã€‚
