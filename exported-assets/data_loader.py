"""
src/data_loader.py
æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æ¨¡å—
"""

import pandas as pd
import numpy as np
from pathlib import Path


class DataLoader:
    """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ç±»"""
    
    def __init__(self, data_path='./data/raw/'):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.data_path = Path(data_path)
        self.train_df = None
        self.test_df = None
        self.features = None
        self.target = None
        
    def load_data(self, train_file='train.csv', test_file='test.csv'):
        """
        åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        
        Args:
            train_file: è®­ç»ƒæ–‡ä»¶å
            test_file: æµ‹è¯•æ–‡ä»¶å
            
        Returns:
            train_df, test_df: è®­ç»ƒå’Œæµ‹è¯•æ•°æ®æ¡†
        """
        try:
            self.train_df = pd.read_csv(self.data_path / train_file)
            self.test_df = pd.read_csv(self.data_path / test_file)
            
            print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸ")
            print(f"  è®­ç»ƒé›†å½¢çŠ¶: {self.train_df.shape}")
            print(f"  æµ‹è¯•é›†å½¢çŠ¶: {self.test_df.shape}")
            
            return self.train_df, self.test_df
            
        except FileNotFoundError as e:
            print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
            print(f"  è¯·ç¡®ä¿ {train_file} å’Œ {test_file} åœ¨ {self.data_path} ç›®å½•ä¸‹")
            raise e
    
    def basic_eda(self, verbose=True):
        """
        åŸºç¡€æ•°æ®æ¢ç´¢åˆ†æ
        
        Args:
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        if self.train_df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_data() åŠ è½½æ•°æ®")
        
        if verbose:
            print("\n" + "="*60)
            print("ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯")
            print("="*60)
            
            print("\n1. æ•°æ®ç»´åº¦")
            print(f"   è®­ç»ƒé›†: {self.train_df.shape}")
            print(f"   æµ‹è¯•é›†: {self.test_df.shape}")
            
            print("\n2. æ•°æ®ç±»å‹")
            print(self.train_df.dtypes.value_counts())
            
            print("\n3. ç¼ºå¤±å€¼ç»Ÿè®¡")
            missing = self.train_df.isnull().sum()
            missing_pct = 100 * missing / len(self.train_df)
            missing_table = pd.DataFrame({
                'ç¼ºå¤±æ•°é‡': missing,
                'ç¼ºå¤±ç™¾åˆ†æ¯”': missing_pct
            })
            missing_table = missing_table[missing_table['ç¼ºå¤±æ•°é‡'] > 0].sort_values(
                'ç¼ºå¤±æ•°é‡', ascending=False
            )
            
            if len(missing_table) > 0:
                print(missing_table.head(10))
            else:
                print("   æ— ç¼ºå¤±å€¼")
            
            print("\n4. ç»Ÿè®¡æè¿°")
            print(self.train_df.describe().T)
        
        return {
            'missing': missing,
            'stats': self.train_df.describe()
        }
    
    def prepare_features(self, target_col='target', exclude_cols=None):
        """
        å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
        
        Args:
            target_col: ç›®æ ‡å˜é‡åˆ—å
            exclude_cols: è¦æ’é™¤çš„åˆ—ååˆ—è¡¨
            
        Returns:
            features: ç‰¹å¾åˆ—è¡¨
            target: ç›®æ ‡å˜é‡å
        """
        if self.train_df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_data() åŠ è½½æ•°æ®")
        
        # é»˜è®¤æ’é™¤çš„åˆ—
        default_exclude = ['id', 'date', target_col]
        if exclude_cols:
            default_exclude.extend(exclude_cols)
        
        # è¯†åˆ«ç‰¹å¾åˆ—
        self.features = [col for col in self.train_df.columns 
                        if col not in default_exclude]
        self.target = target_col
        
        print(f"\nâœ“ ç‰¹å¾å‡†å¤‡å®Œæˆ")
        print(f"  ç‰¹å¾æ•°é‡: {len(self.features)}")
        print(f"  ç›®æ ‡å˜é‡: {self.target}")
        
        if len(self.features) <= 20:
            print(f"  ç‰¹å¾åˆ—è¡¨: {self.features}")
        else:
            print(f"  å‰10ä¸ªç‰¹å¾: {self.features[:10]}")
            print(f"  å10ä¸ªç‰¹å¾: {self.features[-10:]}")
        
        return self.features, self.target
    
    def get_feature_types(self):
        """
        è¯†åˆ«ç‰¹å¾ç±»å‹
        
        Returns:
            dict: åŒ…å«æ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾çš„å­—å…¸
        """
        if self.features is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ prepare_features()")
        
        numeric_features = self.train_df[self.features].select_dtypes(
            include=[np.number]
        ).columns.tolist()
        
        categorical_features = self.train_df[self.features].select_dtypes(
            include=['object', 'category']
        ).columns.tolist()
        
        feature_types = {
            'numeric': numeric_features,
            'categorical': categorical_features
        }
        
        print(f"\nç‰¹å¾ç±»å‹åˆ†æ:")
        print(f"  æ•°å€¼å‹ç‰¹å¾: {len(numeric_features)}")
        print(f"  ç±»åˆ«å‹ç‰¹å¾: {len(categorical_features)}")
        
        return feature_types
    
    def check_data_quality(self):
        """
        æ•°æ®è´¨é‡æ£€æŸ¥
        
        Returns:
            dict: è´¨é‡æ£€æŸ¥ç»“æœ
        """
        if self.train_df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_data() åŠ è½½æ•°æ®")
        
        results = {}
        
        # æ£€æŸ¥é‡å¤è¡Œ
        duplicates = self.train_df.duplicated().sum()
        results['duplicates'] = duplicates
        
        # æ£€æŸ¥æ— ç©·å¤§å€¼
        inf_counts = {}
        for col in self.train_df.select_dtypes(include=[np.number]).columns:
            inf_count = np.isinf(self.train_df[col]).sum()
            if inf_count > 0:
                inf_counts[col] = inf_count
        results['infinity_values'] = inf_counts
        
        # æ£€æŸ¥å¸¸æ•°ç‰¹å¾
        constant_features = []
        for col in self.features if self.features else self.train_df.columns:
            if self.train_df[col].nunique() == 1:
                constant_features.append(col)
        results['constant_features'] = constant_features
        
        print("\n" + "="*60)
        print("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
        print("="*60)
        print(f"é‡å¤è¡Œæ•°: {duplicates}")
        print(f"æ— ç©·å¤§å€¼: {len(inf_counts)} åˆ—")
        print(f"å¸¸æ•°ç‰¹å¾: {len(constant_features)} åˆ—")
        
        if constant_features:
            print(f"  å¸¸æ•°ç‰¹å¾åˆ—è¡¨: {constant_features}")
        
        return results
    
    def save_processed_data(self, output_path='./data/processed/'):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            output_path: è¾“å‡ºè·¯å¾„
        """
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        self.train_df.to_csv(output_path / 'train_processed.csv', index=False)
        self.test_df.to_csv(output_path / 'test_processed.csv', index=False)
        
        print(f"\nâœ“ æ•°æ®å·²ä¿å­˜åˆ° {output_path}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆå§‹åŒ–
    loader = DataLoader(data_path='./data/raw/')
    
    # åŠ è½½æ•°æ®
    train_df, test_df = loader.load_data()
    
    # æ•°æ®æ¢ç´¢
    loader.basic_eda()
    
    # å‡†å¤‡ç‰¹å¾
    features, target = loader.prepare_features()
    
    # è´¨é‡æ£€æŸ¥
    loader.check_data_quality()
    
    # ç‰¹å¾ç±»å‹åˆ†æ
    feature_types = loader.get_feature_types()
