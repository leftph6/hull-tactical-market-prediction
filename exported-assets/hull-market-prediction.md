# Hull Tactical Market Prediction - å®Œæ•´ä»£ç æ¡†æ¶

## 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†æ¨¡å—

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

class DataLoader:
    """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ç±»"""
    
    def __init__(self, data_path='./'):
        self.data_path = data_path
        self.train_df = None
        self.test_df = None
        self.features = None
        self.target = None
        
    def load_data(self):
        """åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        self.train_df = pd.read_csv(f'{self.data_path}/train.csv')
        self.test_df = pd.read_csv(f'{self.data_path}/test.csv')
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {self.train_df.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {self.test_df.shape}")
        print(f"\nè®­ç»ƒé›†åˆ—å:\n{self.train_df.columns.tolist()}")
        
        return self.train_df, self.test_df
    
    def basic_eda(self):
        """åŸºç¡€æ•°æ®æ¢ç´¢"""
        print("\n" + "="*50)
        print("æ•°æ®åŸºæœ¬ä¿¡æ¯")
        print("="*50)
        print(self.train_df.info())
        
        print("\n" + "="*50)
        print("æ•°æ®ç»Ÿè®¡æè¿°")
        print("="*50)
        print(self.train_df.describe())
        
        print("\n" + "="*50)
        print("ç¼ºå¤±å€¼ç»Ÿè®¡")
        print("="*50)
        missing = self.train_df.isnull().sum()
        print(missing[missing > 0])
        
    def prepare_features(self, target_col='target'):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
        # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆæ’é™¤IDå’Œç›®æ ‡åˆ—ï¼‰
        exclude_cols = ['id', 'date', target_col]
        self.features = [col for col in self.train_df.columns 
                        if col not in exclude_cols]
        self.target = target_col
        
        print(f"\nç‰¹å¾æ•°é‡: {len(self.features)}")
        print(f"ç‰¹å¾åˆ—è¡¨: {self.features[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
        
        return self.features, self.target

# åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
loader = DataLoader()
train_df, test_df = loader.load_data()
loader.basic_eda()
features, target = loader.prepare_features()
```

## 2. ç‰¹å¾å·¥ç¨‹æ¨¡å—

```python
class FeatureEngineering:
    """ç‰¹å¾å·¥ç¨‹ç±»"""
    
    def __init__(self, train_df, test_df, features):
        self.train_df = train_df.copy()
        self.test_df = test_df.copy()
        self.features = features
        
    def handle_missing_values(self, strategy='median'):
        """å¤„ç†ç¼ºå¤±å€¼"""
        if strategy == 'median':
            for col in self.features:
                median_val = self.train_df[col].median()
                self.train_df[col].fillna(median_val, inplace=True)
                self.test_df[col].fillna(median_val, inplace=True)
        elif strategy == 'forward_fill':
            self.train_df[self.features] = self.train_df[self.features].fillna(method='ffill')
            self.test_df[self.features] = self.test_df[self.features].fillna(method='ffill')
        
        print(f"ç¼ºå¤±å€¼å¤„ç†å®Œæˆ (ç­–ç•¥: {strategy})")
        return self.train_df, self.test_df
    
    def create_lag_features(self, columns, lags=[1, 2, 3, 5, 10]):
        """åˆ›å»ºæ»åç‰¹å¾"""
        new_features = []
        
        for col in columns:
            if col in self.train_df.columns:
                for lag in lags:
                    new_col = f'{col}_lag_{lag}'
                    self.train_df[new_col] = self.train_df[col].shift(lag)
                    self.test_df[new_col] = self.test_df[col].shift(lag)
                    new_features.append(new_col)
        
        print(f"åˆ›å»ºäº† {len(new_features)} ä¸ªæ»åç‰¹å¾")
        return new_features
    
    def create_rolling_features(self, columns, windows=[5, 10, 20, 30]):
        """åˆ›å»ºæ»šåŠ¨çª—å£ç‰¹å¾"""
        new_features = []
        
        for col in columns:
            if col in self.train_df.columns:
                for window in windows:
                    # æ»šåŠ¨å‡å€¼
                    new_col_mean = f'{col}_roll_mean_{window}'
                    self.train_df[new_col_mean] = self.train_df[col].rolling(window).mean()
                    self.test_df[new_col_mean] = self.test_df[col].rolling(window).mean()
                    new_features.append(new_col_mean)
                    
                    # æ»šåŠ¨æ ‡å‡†å·®
                    new_col_std = f'{col}_roll_std_{window}'
                    self.train_df[new_col_std] = self.train_df[col].rolling(window).std()
                    self.test_df[new_col_std] = self.test_df[col].rolling(window).std()
                    new_features.append(new_col_std)
        
        print(f"åˆ›å»ºäº† {len(new_features)} ä¸ªæ»šåŠ¨çª—å£ç‰¹å¾")
        return new_features
    
    def create_momentum_features(self, columns, periods=[5, 10, 20]):
        """åˆ›å»ºåŠ¨é‡ç‰¹å¾"""
        new_features = []
        
        for col in columns:
            if col in self.train_df.columns:
                for period in periods:
                    new_col = f'{col}_momentum_{period}'
                    self.train_df[new_col] = self.train_df[col] - self.train_df[col].shift(period)
                    self.test_df[new_col] = self.test_df[col] - self.test_df[col].shift(period)
                    new_features.append(new_col)
        
        print(f"åˆ›å»ºäº† {len(new_features)} ä¸ªåŠ¨é‡ç‰¹å¾")
        return new_features
    
    def create_interaction_features(self, feature_pairs):
        """åˆ›å»ºäº¤äº’ç‰¹å¾"""
        new_features = []
        
        for feat1, feat2 in feature_pairs:
            if feat1 in self.train_df.columns and feat2 in self.train_df.columns:
                new_col = f'{feat1}_x_{feat2}'
                self.train_df[new_col] = self.train_df[feat1] * self.train_df[feat2]
                self.test_df[new_col] = self.test_df[feat1] * self.test_df[feat2]
                new_features.append(new_col)
        
        print(f"åˆ›å»ºäº† {len(new_features)} ä¸ªäº¤äº’ç‰¹å¾")
        return new_features

# ä½¿ç”¨ç‰¹å¾å·¥ç¨‹
fe = FeatureEngineering(train_df, test_df, features)
train_df, test_df = fe.handle_missing_values(strategy='median')

# å¯é€‰ï¼šåˆ›å»ºé¢å¤–ç‰¹å¾
# lag_features = fe.create_lag_features(features[:5], lags=[1, 2, 3, 5])
# rolling_features = fe.create_rolling_features(features[:5], windows=[5, 10, 20])
# momentum_features = fe.create_momentum_features(features[:5], periods=[5, 10, 20])
```

## 3. æ¨¡å‹æ„å»ºæ¨¡å—

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from lightgbm import LGBMRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

class ModelBuilder:
    """æ¨¡å‹æ„å»ºå’Œè®­ç»ƒç±»"""
    
    def __init__(self, train_df, features, target):
        self.train_df = train_df
        self.features = features
        self.target = target
        self.models = {}
        self.predictions = {}
        
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        self.train_df = self.train_df.dropna(subset=self.features + [self.target])
        
        X = self.train_df[self.features].values
        y = self.train_df[self.target].values
        
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        return X, y
    
    def get_baseline_models(self):
        """è·å–åŸºç¡€æ¨¡å‹é›†åˆ"""
        models = {
            'ridge': Ridge(alpha=1.0, random_state=42),
            'lasso': Lasso(alpha=0.01, random_state=42),
            'elasticnet': ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42),
            'rf': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                min_samples_split=10,
                random_state=42,
                n_jobs=-1
            ),
            'gbm': GradientBoostingRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42
            ),
            'lgbm': LGBMRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                num_leaves=31,
                random_state=42,
                verbose=-1
            ),
            'xgb': XGBRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=42,
                tree_method='hist'
            ),
            'catboost': CatBoostRegressor(
                iterations=100,
                learning_rate=0.1,
                depth=5,
                random_state=42,
                verbose=False
            )
        }
        return models
    
    def time_series_cv(self, X, y, n_splits=5):
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯"""
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        models = self.get_baseline_models()
        cv_scores = {}
        
        print("\n" + "="*50)
        print("æ—¶é—´åºåˆ—äº¤å‰éªŒè¯")
        print("="*50)
        
        for model_name, model in models.items():
            scores = []
            
            for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
                
                # æ ‡å‡†åŒ–
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_val_scaled = scaler.transform(X_val)
                
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train_scaled, y_train)
                
                # é¢„æµ‹
                y_pred = model.predict(X_val_scaled)
                
                # è®¡ç®—RMSE
                rmse = np.sqrt(mean_squared_error(y_val, y_pred))
                scores.append(rmse)
            
            cv_scores[model_name] = {
                'mean_rmse': np.mean(scores),
                'std_rmse': np.std(scores),
                'scores': scores
            }
            
            print(f"{model_name:15s} - RMSE: {np.mean(scores):.6f} (+/- {np.std(scores):.6f})")
        
        self.cv_scores = cv_scores
        return cv_scores
    
    def train_final_models(self, X, y):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        print("\n" + "="*50)
        print("è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        print("="*50)
        
        # æ•°æ®æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(X)
        
        models = self.get_baseline_models()
        
        for model_name, model in models.items():
            print(f"è®­ç»ƒ {model_name}...")
            model.fit(X_scaled, y)
            self.models[model_name] = model
        
        print(f"è®­ç»ƒå®Œæˆï¼å…±è®­ç»ƒäº† {len(self.models)} ä¸ªæ¨¡å‹")
        return self.models
    
    def predict(self, test_df):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_df = test_df.dropna(subset=self.features)
        X_test = test_df[self.features].values
        X_test_scaled = self.scaler.transform(X_test)
        
        predictions = {}
        
        for model_name, model in self.models.items():
            predictions[model_name] = model.predict(X_test_scaled)
        
        self.predictions = predictions
        return predictions

# ä½¿ç”¨æ¨¡å‹æ„å»ºå™¨
builder = ModelBuilder(train_df, features, target)
X, y = builder.prepare_data()

# äº¤å‰éªŒè¯
cv_scores = builder.time_series_cv(X, y, n_splits=5)

# è®­ç»ƒæœ€ç»ˆæ¨¡å‹
models = builder.train_final_models(X, y)
```

## 4. é›†æˆå­¦ä¹ æ¨¡å—

```python
class EnsembleModel:
    """æ¨¡å‹é›†æˆç±»"""
    
    def __init__(self, predictions_dict):
        self.predictions = predictions_dict
        self.weights = None
        
    def simple_average(self):
        """ç®€å•å¹³å‡"""
        pred_array = np.array(list(self.predictions.values()))
        return np.mean(pred_array, axis=0)
    
    def weighted_average(self, weights=None):
        """åŠ æƒå¹³å‡"""
        if weights is None:
            # é»˜è®¤æƒé‡ï¼šåŸºäºäº¤å‰éªŒè¯æ€§èƒ½
            weights = self.calculate_optimal_weights()
        
        self.weights = weights
        pred_array = np.array(list(self.predictions.values()))
        return np.average(pred_array, axis=0, weights=weights)
    
    def calculate_optimal_weights(self):
        """åŸºäºäº¤å‰éªŒè¯åˆ†æ•°è®¡ç®—æœ€ä¼˜æƒé‡"""
        # è¿™é‡Œéœ€è¦cv_scoresï¼Œç®€åŒ–å¤„ç†ï¼šå‡ç­‰æƒé‡
        n_models = len(self.predictions)
        return np.ones(n_models) / n_models
    
    def median_ensemble(self):
        """ä¸­ä½æ•°é›†æˆ"""
        pred_array = np.array(list(self.predictions.values()))
        return np.median(pred_array, axis=0)
    
    def rank_average(self):
        """æ’åå¹³å‡"""
        pred_array = np.array(list(self.predictions.values()))
        
        # å°†æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹è½¬æ¢ä¸ºæ’å
        ranked_preds = np.zeros_like(pred_array)
        for i in range(pred_array.shape[0]):
            ranked_preds[i] = np.argsort(np.argsort(pred_array[i]))
        
        # å¹³å‡æ’å
        avg_rank = np.mean(ranked_preds, axis=0)
        
        # å°†æ’åè½¬æ¢å›é¢„æµ‹å€¼ï¼ˆä½¿ç”¨ç®€å•å¹³å‡çš„å°ºåº¦ï¼‰
        simple_avg = self.simple_average()
        sorted_indices = np.argsort(avg_rank)
        sorted_values = np.sort(simple_avg)
        result = np.zeros_like(simple_avg)
        result[sorted_indices] = sorted_values
        
        return result

# ä½¿ç”¨é›†æˆæ¨¡å‹
predictions = builder.predict(test_df)
ensemble = EnsembleModel(predictions)

# ä¸åŒçš„é›†æˆç­–ç•¥
pred_simple = ensemble.simple_average()
pred_weighted = ensemble.weighted_average()
pred_median = ensemble.median_ensemble()
pred_rank = ensemble.rank_average()

print("\né›†æˆé¢„æµ‹å®Œæˆï¼")
print(f"ç®€å•å¹³å‡é¢„æµ‹èŒƒå›´: [{pred_simple.min():.6f}, {pred_simple.max():.6f}]")
print(f"åŠ æƒå¹³å‡é¢„æµ‹èŒƒå›´: [{pred_weighted.min():.6f}, {pred_weighted.max():.6f}]")
```

## 5. æäº¤æ–‡ä»¶ç”Ÿæˆæ¨¡å—

```python
class SubmissionGenerator:
    """ç”Ÿæˆæäº¤æ–‡ä»¶"""
    
    def __init__(self, test_df, predictions):
        self.test_df = test_df
        self.predictions = predictions
        
    def create_submission(self, pred_values, filename='submission.csv'):
        """åˆ›å»ºæäº¤æ–‡ä»¶"""
        submission = pd.DataFrame({
            'id': self.test_df['id'].values[:len(pred_values)],
            'target': pred_values
        })
        
        submission.to_csv(filename, index=False)
        print(f"\næäº¤æ–‡ä»¶å·²ä¿å­˜: {filename}")
        print(f"æäº¤æ–‡ä»¶å½¢çŠ¶: {submission.shape}")
        print(f"\nå‰5è¡Œé¢„è§ˆ:")
        print(submission.head())
        
        return submission
    
    def create_multiple_submissions(self, predictions_dict):
        """åˆ›å»ºå¤šä¸ªæäº¤æ–‡ä»¶"""
        for name, pred in predictions_dict.items():
            filename = f'submission_{name}.csv'
            self.create_submission(pred, filename)

# ç”Ÿæˆæäº¤æ–‡ä»¶
gen = SubmissionGenerator(test_df, predictions)

# ç”Ÿæˆä¸åŒé›†æˆç­–ç•¥çš„æäº¤
ensemble_predictions = {
    'simple_avg': pred_simple,
    'weighted_avg': pred_weighted,
    'median': pred_median,
    'rank_avg': pred_rank
}

gen.create_multiple_submissions(ensemble_predictions)

# ä¹Ÿå¯ä»¥ä¸ºå•ä¸ªæ¨¡å‹ç”Ÿæˆæäº¤
# gen.create_submission(predictions['lgbm'], 'submission_lgbm.csv')
```

## 6. è¯„ä¼°å’Œåˆ†ææ¨¡å—

```python
class ModelAnalysis:
    """æ¨¡å‹åˆ†æç±»"""
    
    def __init__(self, train_df, features, target):
        self.train_df = train_df
        self.features = features
        self.target = target
        
    def feature_importance_analysis(self, model, model_name='model', top_n=20):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1][:top_n]
            
            plt.figure(figsize=(12, 6))
            plt.title(f'Top {top_n} Feature Importances - {model_name}')
            plt.bar(range(top_n), importances[indices])
            plt.xticks(range(top_n), [self.features[i] for i in indices], rotation=90)
            plt.tight_layout()
            plt.savefig(f'feature_importance_{model_name}.png', dpi=150, bbox_inches='tight')
            plt.show()
            
            # æ‰“å°é‡è¦ç‰¹å¾
            print(f"\n{model_name} - Top {top_n} æœ€é‡è¦ç‰¹å¾:")
            for i in range(top_n):
                idx = indices[i]
                print(f"{i+1}. {self.features[idx]:30s} : {importances[idx]:.6f}")
        else:
            print(f"{model_name} ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
    
    def plot_predictions_distribution(self, predictions_dict):
        """ç»˜åˆ¶é¢„æµ‹åˆ†å¸ƒ"""
        plt.figure(figsize=(15, 10))
        
        for i, (name, pred) in enumerate(predictions_dict.items(), 1):
            plt.subplot(3, 3, i)
            plt.hist(pred, bins=50, alpha=0.7, edgecolor='black')
            plt.title(f'{name} - é¢„æµ‹åˆ†å¸ƒ')
            plt.xlabel('é¢„æµ‹å€¼')
            plt.ylabel('é¢‘æ•°')
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('predictions_distribution.png', dpi=150, bbox_inches='tight')
        plt.show()
    
    def correlation_analysis(self):
        """ç›¸å…³æ€§åˆ†æ"""
        # è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§
        correlations = self.train_df[self.features + [self.target]].corr()[self.target].drop(self.target)
        correlations = correlations.sort_values(ascending=False)
        
        print("\n" + "="*50)
        print("ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§ (Top 20)")
        print("="*50)
        print(correlations.head(20))
        
        # å¯è§†åŒ–
        plt.figure(figsize=(12, 8))
        correlations.head(20).plot(kind='barh')
        plt.title('Top 20 ç‰¹å¾ä¸ç›®æ ‡çš„ç›¸å…³æ€§')
        plt.xlabel('ç›¸å…³ç³»æ•°')
        plt.tight_layout()
        plt.savefig('feature_correlation.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        return correlations

# ä½¿ç”¨åˆ†ææ¨¡å—
analyzer = ModelAnalysis(train_df, features, target)

# ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä»¥LightGBMä¸ºä¾‹ï¼‰
if 'lgbm' in models:
    analyzer.feature_importance_analysis(models['lgbm'], 'LightGBM', top_n=20)

# é¢„æµ‹åˆ†å¸ƒ
all_predictions = {**predictions, **ensemble_predictions}
analyzer.plot_predictions_distribution(all_predictions)

# ç›¸å…³æ€§åˆ†æ
correlations = analyzer.correlation_analysis()
```

## 7. è¶…å‚æ•°è°ƒä¼˜æ¨¡å—

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

class HyperparameterTuning:
    """è¶…å‚æ•°è°ƒä¼˜ç±»"""
    
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.best_params = {}
        
    def tune_lgbm(self, n_iter=20):
        """è°ƒä¼˜LightGBM"""
        param_dist = {
            'n_estimators': randint(100, 500),
            'learning_rate': uniform(0.01, 0.3),
            'max_depth': randint(3, 10),
            'num_leaves': randint(20, 100),
            'min_child_samples': randint(10, 50),
            'subsample': uniform(0.6, 0.4),
            'colsample_bytree': uniform(0.6, 0.4),
            'reg_alpha': uniform(0, 1),
            'reg_lambda': uniform(0, 1)
        }
        
        lgbm = LGBMRegressor(random_state=42, verbose=-1)
        
        tscv = TimeSeriesSplit(n_splits=3)
        
        random_search = RandomizedSearchCV(
            lgbm, param_dist, n_iter=n_iter, 
            cv=tscv, scoring='neg_mean_squared_error',
            random_state=42, n_jobs=-1, verbose=1
        )
        
        print("\nå¼€å§‹LightGBMè¶…å‚æ•°è°ƒä¼˜...")
        random_search.fit(self.X, self.y)
        
        self.best_params['lgbm'] = random_search.best_params_
        print(f"\nLightGBMæœ€ä½³å‚æ•°:")
        print(random_search.best_params_)
        print(f"æœ€ä½³RMSE: {np.sqrt(-random_search.best_score_):.6f}")
        
        return random_search.best_estimator_
    
    def tune_xgb(self, n_iter=20):
        """è°ƒä¼˜XGBoost"""
        param_dist = {
            'n_estimators': randint(100, 500),
            'learning_rate': uniform(0.01, 0.3),
            'max_depth': randint(3, 10),
            'min_child_weight': randint(1, 10),
            'subsample': uniform(0.6, 0.4),
            'colsample_bytree': uniform(0.6, 0.4),
            'gamma': uniform(0, 0.5),
            'reg_alpha': uniform(0, 1),
            'reg_lambda': uniform(0, 2)
        }
        
        xgb = XGBRegressor(random_state=42, tree_method='hist')
        
        tscv = TimeSeriesSplit(n_splits=3)
        
        random_search = RandomizedSearchCV(
            xgb, param_dist, n_iter=n_iter,
            cv=tscv, scoring='neg_mean_squared_error',
            random_state=42, n_jobs=-1, verbose=1
        )
        
        print("\nå¼€å§‹XGBoostè¶…å‚æ•°è°ƒä¼˜...")
        random_search.fit(self.X, self.y)
        
        self.best_params['xgb'] = random_search.best_params_
        print(f"\nXGBoostæœ€ä½³å‚æ•°:")
        print(random_search.best_params_)
        print(f"æœ€ä½³RMSE: {np.sqrt(-random_search.best_score_):.6f}")
        
        return random_search.best_estimator_

# è¶…å‚æ•°è°ƒä¼˜ç¤ºä¾‹ï¼ˆå¯é€‰ï¼Œè€—æ—¶è¾ƒé•¿ï¼‰
# tuner = HyperparameterTuning(X, y)
# best_lgbm = tuner.tune_lgbm(n_iter=20)
# best_xgb = tuner.tune_xgb(n_iter=20)
```

## 8. å®Œæ•´å·¥ä½œæµç¨‹

```python
def main_pipeline():
    """å®Œæ•´çš„å·¥ä½œæµç¨‹"""
    
    print("="*70)
    print("Hull Tactical Market Prediction - å®Œæ•´æµç¨‹")
    print("="*70)
    
    # 1. æ•°æ®åŠ è½½
    print("\næ­¥éª¤ 1: æ•°æ®åŠ è½½")
    print("-"*70)
    loader = DataLoader()
    train_df, test_df = loader.load_data()
    loader.basic_eda()
    features, target = loader.prepare_features()
    
    # 2. ç‰¹å¾å·¥ç¨‹
    print("\næ­¥éª¤ 2: ç‰¹å¾å·¥ç¨‹")
    print("-"*70)
    fe = FeatureEngineering(train_df, test_df, features)
    train_df, test_df = fe.handle_missing_values(strategy='median')
    
    # å¯é€‰ï¼šåˆ›å»ºé¢å¤–ç‰¹å¾
    # new_features = []
    # new_features += fe.create_lag_features(features[:5], lags=[1, 2, 3])
    # new_features += fe.create_rolling_features(features[:5], windows=[5, 10, 20])
    # features = features + new_features
    
    # 3. æ¨¡å‹è®­ç»ƒ
    print("\næ­¥éª¤ 3: æ¨¡å‹è®­ç»ƒ")
    print("-"*70)
    builder = ModelBuilder(train_df, features, target)
    X, y = builder.prepare_data()
    
    # äº¤å‰éªŒè¯
    cv_scores = builder.time_series_cv(X, y, n_splits=5)
    
    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    models = builder.train_final_models(X, y)
    
    # 4. é¢„æµ‹
    print("\næ­¥éª¤ 4: ç”Ÿæˆé¢„æµ‹")
    print("-"*70)
    predictions = builder.predict(test_df)
    
    # 5. é›†æˆ
    print("\næ­¥éª¤ 5: æ¨¡å‹é›†æˆ")
    print("-"*70)
    ensemble = EnsembleModel(predictions)
    pred_simple = ensemble.simple_average()
    pred_weighted = ensemble.weighted_average()
    pred_median = ensemble.median_ensemble()
    
    # 6. ç”Ÿæˆæäº¤
    print("\næ­¥éª¤ 6: ç”Ÿæˆæäº¤æ–‡ä»¶")
    print("-"*70)
    gen = SubmissionGenerator(test_df, predictions)
    
    ensemble_predictions = {
        'simple_avg': pred_simple,
        'weighted_avg': pred_weighted,
        'median': pred_median
    }
    
    gen.create_multiple_submissions(ensemble_predictions)
    
    # 7. åˆ†æ
    print("\næ­¥éª¤ 7: æ¨¡å‹åˆ†æ")
    print("-"*70)
    analyzer = ModelAnalysis(train_df, features, target)
    
    if 'lgbm' in models:
        analyzer.feature_importance_analysis(models['lgbm'], 'LightGBM', top_n=20)
    
    all_predictions = {**predictions, **ensemble_predictions}
    analyzer.plot_predictions_distribution(all_predictions)
    
    print("\n" + "="*70)
    print("æµç¨‹å®Œæˆï¼")
    print("="*70)

# è¿è¡Œå®Œæ•´æµç¨‹
if __name__ == '__main__':
    main_pipeline()
```

## 9. è°ƒå‚å»ºè®®

### å¿«é€Ÿè°ƒå‚æ¸…å•

#### æ•°æ®é¢„å¤„ç†
- [ ] å°è¯•ä¸åŒçš„ç¼ºå¤±å€¼å¡«å……ç­–ç•¥ï¼ˆmedian, mean, forward_fillï¼‰
- [ ] å°è¯•ä¸åŒçš„ç‰¹å¾æ ‡å‡†åŒ–æ–¹æ³•ï¼ˆStandardScaler, RobustScaler, MinMaxScalerï¼‰
- [ ] å¤„ç†å¼‚å¸¸å€¼ï¼ˆIQRæ–¹æ³•ã€Z-scoreï¼‰

#### ç‰¹å¾å·¥ç¨‹
- [ ] è°ƒæ•´æ»åç‰¹å¾çš„æ»åæœŸæ•°ï¼š`lags=[1,2,3,5,10,20,30]`
- [ ] è°ƒæ•´æ»šåŠ¨çª—å£å¤§å°ï¼š`windows=[3,5,7,10,15,20,30,60]`
- [ ] åˆ›å»ºæ›´å¤šåŠ¨é‡ç‰¹å¾å’Œæ³¢åŠ¨ç‡ç‰¹å¾
- [ ] å°è¯•ç‰¹å¾äº¤äº’ï¼ˆå¤šé¡¹å¼ç‰¹å¾ã€æ¯”ç‡ç‰¹å¾ï¼‰
- [ ] ç‰¹å¾é€‰æ‹©ï¼ˆåŸºäºé‡è¦æ€§ã€ç›¸å…³æ€§ã€é€’å½’ç‰¹å¾æ¶ˆé™¤ï¼‰

#### æ¨¡å‹è¶…å‚æ•°ï¼ˆLightGBM - æ¨èé‡ç‚¹è°ƒä¼˜ï¼‰
```python
lgbm_params = {
    'n_estimators': [100, 200, 300, 500],      # æ ‘çš„æ•°é‡
    'learning_rate': [0.01, 0.05, 0.1, 0.2],   # å­¦ä¹ ç‡
    'max_depth': [3, 5, 7, 10],                 # æ ‘çš„æ·±åº¦
    'num_leaves': [15, 31, 63, 127],            # å¶å­èŠ‚ç‚¹æ•°
    'min_child_samples': [10, 20, 30, 50],      # æœ€å°æ ·æœ¬æ•°
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],     # æ ·æœ¬é‡‡æ ·æ¯”ä¾‹
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # ç‰¹å¾é‡‡æ ·æ¯”ä¾‹
    'reg_alpha': [0, 0.1, 0.5, 1.0],            # L1æ­£åˆ™åŒ–
    'reg_lambda': [0, 0.1, 0.5, 1.0, 2.0]       # L2æ­£åˆ™åŒ–
}
```

#### æ¨¡å‹è¶…å‚æ•°ï¼ˆXGBoostï¼‰
```python
xgb_params = {
    'n_estimators': [100, 200, 300, 500],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'min_child_weight': [1, 3, 5, 7],
    'subsample': [0.6, 0.7, 0.8, 0.9],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2, 0.3],
    'reg_alpha': [0, 0.1, 0.5, 1.0],
    'reg_lambda': [0, 0.5, 1.0, 2.0]
}
```

#### é›†æˆç­–ç•¥
- [ ] è°ƒæ•´é›†æˆæƒé‡ï¼ˆåŸºäºCVåˆ†æ•°ï¼‰
- [ ] å°è¯•Stackingï¼ˆä½¿ç”¨çº¿æ€§æ¨¡å‹ä½œä¸ºå…ƒå­¦ä¹ å™¨ï¼‰
- [ ] å°è¯•Blendingï¼ˆåœ¨éªŒè¯é›†ä¸Šè®­ç»ƒå…ƒæ¨¡å‹ï¼‰
- [ ] å¤šå±‚Stacking

#### äº¤å‰éªŒè¯
- [ ] è°ƒæ•´äº¤å‰éªŒè¯æŠ˜æ•°ï¼š`n_splits=[3, 5, 10]`
- [ ] å°è¯•ä¸åŒçš„éªŒè¯ç­–ç•¥ï¼ˆKFold, StratifiedKFold, PurgedKFoldï¼‰
- [ ] æ³¨æ„é¿å…æ•°æ®æ³„æ¼ï¼ˆæ—¶é—´åºåˆ—ç‰¹æ€§ï¼‰

### æ”¹è¿›ä¼˜å…ˆçº§ï¼ˆæ¨èé¡ºåºï¼‰

1. **é«˜ä¼˜å…ˆçº§**ï¼ˆæœ€å¯èƒ½æå‡æ€§èƒ½ï¼‰
   - ç‰¹å¾å·¥ç¨‹ï¼šæ»åç‰¹å¾ã€æ»šåŠ¨ç»Ÿè®¡
   - LightGBMè°ƒå‚ï¼šlearning_rate, n_estimators, max_depth
   - é›†æˆç­–ç•¥ï¼šåŠ æƒå¹³å‡ã€Stacking

2. **ä¸­ä¼˜å…ˆçº§**
   - å¼‚å¸¸å€¼å¤„ç†
   - ç‰¹å¾é€‰æ‹©
   - XGBoostè°ƒå‚
   - ä¸åŒçš„ç¼ºå¤±å€¼å¡«å……ç­–ç•¥

3. **ä½ä¼˜å…ˆçº§**ï¼ˆå¯èƒ½æå‡è¾ƒå°ï¼‰
   - å¤æ‚çš„ç‰¹å¾äº¤äº’
   - æ·±åº¦å­¦ä¹ æ¨¡å‹
   - æç«¯çš„è¿‡é‡‡æ ·/æ¬ é‡‡æ ·

## 10. å¸¸è§é—®é¢˜å’Œæ³¨æ„äº‹é¡¹

### æ—¶é—´åºåˆ—æ•°æ®å¤„ç†
- âš ï¸ é¿å…æœªæ¥ä¿¡æ¯æ³„æ¼ï¼ˆä¸è¦ä½¿ç”¨æœªæ¥çš„æ•°æ®é¢„æµ‹è¿‡å»ï¼‰
- âœ… ä½¿ç”¨TimeSeriesSplitè¿›è¡Œäº¤å‰éªŒè¯
- âœ… æŒ‰æ—¶é—´é¡ºåºåˆ›å»ºè®­ç»ƒ/éªŒè¯é›†

### è¯„ä¼°æŒ‡æ ‡
- æœ¬æ¯”èµ›ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡éœ€è¦æŸ¥çœ‹æ¯”èµ›å®˜æ–¹è¯´æ˜
- å¯èƒ½æ˜¯RMSEã€MAEæˆ–è‡ªå®šä¹‰æŒ‡æ ‡
- ç¡®ä¿æœ¬åœ°éªŒè¯æŒ‡æ ‡ä¸çº¿ä¸Šä¸€è‡´

### æäº¤æ ¼å¼
- ç¡®ä¿IDåˆ—ä¸test.csvå®Œå…¨åŒ¹é…
- æ£€æŸ¥é¢„æµ‹å€¼èŒƒå›´æ˜¯å¦åˆç†
- é¿å…NaNå’ŒInfå€¼

### æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨`n_jobs=-1`å¹¶è¡Œè®¡ç®—
- å¯¹äºå¤§æ•°æ®é›†ï¼Œè€ƒè™‘å¢é‡å­¦ä¹ 
- ä½¿ç”¨GPUåŠ é€Ÿï¼ˆXGBoost, LightGBMï¼‰

---

## ä½¿ç”¨è¯´æ˜

1. **å®‰è£…ä¾èµ–**
```bash
pip install pandas numpy scikit-learn lightgbm xgboost catboost matplotlib seaborn
```

2. **æ•°æ®å‡†å¤‡**
- å°†train.csvå’Œtest.csvæ”¾åœ¨å·¥ä½œç›®å½•
- æˆ–ä¿®æ”¹`DataLoader`ä¸­çš„è·¯å¾„

3. **è¿è¡Œæµç¨‹**
```python
# å®Œæ•´è¿è¡Œ
main_pipeline()

# æˆ–åˆ†æ­¥è¿è¡Œ
# ... ï¼ˆä½¿ç”¨ä¸Šé¢çš„å„ä¸ªæ¨¡å—ï¼‰
```

4. **è°ƒå‚å»ºè®®**
- å…ˆç”¨é»˜è®¤å‚æ•°è·‘é€šæµç¨‹
- åˆ†æç‰¹å¾é‡è¦æ€§
- é‡ç‚¹è°ƒä¼˜æœ€é‡è¦çš„å‡ ä¸ªæ¨¡å‹
- å°è¯•ä¸åŒçš„é›†æˆç­–ç•¥

ç¥æ¯”èµ›é¡ºåˆ©ï¼ğŸš€
